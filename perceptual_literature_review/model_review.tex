This section documents a review of perceptual models that are promising for use in perceptual sound zone
algorithms.

Especially promising are models that attach some ``score'' or ``rating'' to the perceptual quality of input signals.
These ratings can be used in algorithms to obtain an optimal rating through optimization.
In addition to this, they can also be used to quantify the quality of the algorithms in \autoref{ch:results}.
As such, the focus of the literature review is not on the latest findings in the field of psycho-acoustics or models that most accurately emulate the behavior of the human ear but rather on models that quantify a perceptual quality.

In addition to this, the review also focuses on the optimization tractability of the models.
This information is used in \autoref{ch:perceptual:selection} to motivate the use of the ``Par distortion detectability'' perceptual model in the proposed perceptual sound zone algorithm.

To this end, in this section, two categories of perceptual models are considered.
First in \autoref{ch:perceptual:review:objective}, ``objective measures'' are discussed.
These are models which attempt to predict the perceptual quality ratings from listening tests. 
Next, perceptual models from ``audio coding'' are discussed in \autoref{ch:perceptual:review:audio_coding}.
These models are typically used to quantify how audible audio compression artifacts are.


\subsection{Review of Objective Measures}
\label{ch:perceptual:review:objective}
In order to objectively determine the perceived quality of audio, one approach is to use listening tests.
These are tests in which subjects are asked to rate a property (or properties) of a set of audio stimuli.
One example where listening tests are used is for the evaluation of speech intelligibility of hearing aids~\cite{taal2011algorithm}.
Another example is determining which loudspeaker has higher perceived sound quality.

Performing listening tests is, however, often cumbersome due to the large amount of human labor involved.
This motivates the use of objective quality measures, which attempt to predict the outcomes of these objective listening tests.
This is very useful for algorithm developers, as they can get an indication of how well they are doing
without having to perform a labor-intensive listening test~\cite{taal2011algorithm}.

Note, however, that an objective quality measure does not replace a listening test: it can only be used to give an 
indication.
Findings should always be confirmed with listening tests.

The objective measures that are considered in this review take a reference and degraded audio stimuli as inputs.
Most of the discussed models take the following approach.
First, input stimuli are converted to their so-called internal representations, which models how the 
human auditory system transforms the stimuli.
Various features are then derived from this internal representation.
The features are then mapped to a prediction of the results of a listening test.

These objective quality measures are promising for integration into sound zone algorithms as they summarize the 
quality of a signal into a single value, which can be potentially optimized. 
It stands to reason that if an objective quality measure correlates with audio quality, optimizing over such a measure
could improve the sound quality of sound zone algorithms.

As such, this section explores various objective measures.
This is done by considering three classes of different objective measures: measures that quantify the quality and intelligibility of speech audio and measures for the general quality of audio. 

\subsubsection{Review of Objective Speech Quality Measures}
There have been a number of attempts to create objective measures to quantify the perceived quality of speech.
In this section, three objective speech quality measures are discussed.
Of these three measures, the Perceptual Evaluation of Speech Quality (PESQ) is used in the evaluation of the proposed perceptual sound zone algorithm in \autoref{ch:results}.

\begin{itemize}
    \item 
    Perceptual Evaluation of Speech Quality (PESQ)~\cite{rix2001perceptual} is a measure that attempts to determine the perceived quality of speech.
    It was standardized by the International Telecommunication Union (ITU-T) in 2001.

    PESQ is computed by first applying an auditory transform that maps the reference and degraded speech into a time-frequency representation that models the perceived loudness of the signals.
    From this internal representation, so-called symmetric and asymmetric disturbances are determined by computing differences between the time-frequency bins of the reference and degraded speech. 
    A non-linear average is then taken to obtain the average disturbance per time bin.
    These averaged disturbances are then mapped to the outcomes of listening test outcomes through linear 
    combination~\cite{rix2001perceptual}.

    \item
    The Perceptual Objective Listening Quality Assessment (POLQA)~\cite{beerends2013perceptual} is a speech quality measure that was standardized by the International Telecommunication Union (ITU-T) in 2011. 
    It was intended to be the successor of PESQ, with the improvement of having more accurate predictions on a broader range of distortions.

    POLQA works with a similar internal representation to PESQ but computes distortion in a different way as to be capable of handling global temporal compression and expansions~\cite{beerends2013perceptual}.

    \item
    Virtual Speech Quality Objective Listener (ViSQOL)~\cite{hines2012visqol,chinen2020visqol} is a measure developed in 2012 in a collaboration between Trinity College and Google.

    ViSQOL uses a different internal representation than PESQ and POLQA as it uses neurograms rather than loudness representations. 
    The neurograms are then compared through the Neurogram Similarly Index Measure (NSIM).
    Neurograms contain the neural firing activity of the auditory nerve in time-frequency bins, and NSIM determines how similar the firing patterns of two neurograms are.
    This similarity is then related to the outcomes of listening tests through a laplacian fit~\cite{hines2012visqol}, which is then used to make predictions.

\end{itemize}
In general, PESQ, POLQA, and ViSQOL require many steps to compute and are difficult to optimize for due to 
conditional branches within the algorithms and many non-differentiable steps such as clipping~\cite{rix2001perceptual,beerends2013perceptual,hines2012visqol}. 
Some attempts have been made, however, to reformulate PESQ in order to make it more tractable for optimization 
by approximating the disturbances by other functions~\cite{kim2019end}.

\subsubsection{Review of Objective Speech Intelligibility Measures}
Intelligibility of speech is defined as the percentage of words identified correctly given a degraded speech signal.
Objective speech intelligibility measures seek to predict this percentage.
In this section, the Short-Time Objective Intelligibility (STOI)~\cite{taal2011algorithm} measure and the 
Speech Intelligibility In Bits (SIIB)~\cite{van2017instrumental} measure are discussed.
Both measures are used in the evaluation of the proposed perceptual sound zone algorithm in \autoref{ch:results}.

\begin{itemize}
    \item 
    The Short-Time Objective Intelligibility (STOI)~\cite{taal2011algorithm} was proposed by Taal et al. 
    in 2011 as a speech intelligibility measure that could make accurate predictions for speech signals degraded by time-frequency weighted distortions.

    For its internal representation, it finds a time-frequency internal representation through filtering the input stimuli with a filter bank consisting of $1/3$ octave bands, and then segmented the filter taps into short time frames.
    Silent bins that do not contain speech are removed, and clipping is applied to limit the 
    effect of one severely degraded time-frequency bin.
    The average correlation coefficient between the time-frequency bins of the internal representation of the reference and degraded segments is then computed and averaged over all bins to determine the intelligibility~\cite{taal2011algorithm}.

    \item 
    The Speech Intelligibility In Bits (SIIB)~\cite{van2017instrumental} was introduced by Van Kuyk et al.
    in 2017 as a speech intelligibility measure that could be motivated through
    the mutual information rate from information theory.
    As such, SIIB is given in bits.

    The idea behind SIIB is that the intelligibility of speech is related to the information shared between 
    intended and degraded speech.
    SIIB models how the reference speech signal transforms to the degraded speech signal as a transmission channel.
    Among other aspects, this transmission channel includes a model of the human auditory system~\cite{van2017instrumental}.
    This communication channel is then used to compute the mutual information rate.
\end{itemize}

Both STOI and SIIB are difficult to optimize for directly. 
In STOI, the removal of silent regions and the clipping operator are non-differentiable operations.
Furthermore, the computation of the correlation coefficient is a non-convex function of the degraded speech~\cite{taal2011algorithm}.
SIIB is in general non-convex and non-differentiable as it uses the Karhunen-Lo\`eve transform and a 
K-nearest neighbor estimator to compute the mutual information rate~\cite{van2017instrumental}.
However, if the communication channel is approximated as Gaussian, the mutual information can be computed in closed form,
and SIIB becomes a differentiable measure~\cite{van2017instrumental}.

\subsubsection{Review of Objective Audio Quality Measures}
The previous objective quality measures are both intended for evaluating speech.
In this section, two objective quality measures are discussed that are designed for evaluating the perceived quality of any audio stimuli.
\begin{itemize}
    \item 
    The Perceptual Evaluation of Audio Quality (PEAQ)~\cite{thiede2000peaq} is an audio quality measure standardized by the International Telecommunication Union (ITU-T).

    PEAQ estimates a quality grade by first computing an internal representation of the reference and degraded audio signals.
    This results in a time-frequency representation of the input stimuli from which several perceptually relevant features, referred to by PEAQ as Model Output Variables (MOVs), are extracted.
    An example of these MOVs is the loudness of the noise or the bandwidth of the input stimuli.
    These MOVs are then mapped to the final audio quality grade through a neural network~\cite{thiede2000peaq}.
    \item 
    In 2015, it was found that, with some adjustments, the previously discussed ViSQOL measure
    could be used to determine audio quality. 
    This resulted in a new measure, ViSQOLAudio~\cite{hines2015visqolaudio}.

    Among the adjustments were the removal of the voice activity detector included in ViSQOL and the use of a larger bandwidth to cover the entire spectrum of hearing from 50 Hz to 20000 Hz, rather than just the bandwidth of speech~\cite{hines2015visqolaudio}.
\end{itemize}

PEAQ and ViSQOLAudio are both difficult to optimize.
A number of the MOVs computed in PEAQ, such as the partial noise loudness, are non-differentiable~\cite{thiede2000peaq}.
As ViSQOLAudio is similar to ViSQOL with some minor adjustments, it is similarly challenging to optimize.

\subsubsection{Review of the Distraction Model}
In an elicitation study performed by Francombe et al. 
in 2014~\cite{francombe2014elicitation}, ``distraction'' was determined to be the keyword that best describes the perceptual experience of interfering audio programs.
Further research led to the proposal of a ``distraction model'', which is capable of estimating how distracting an interferer stimulus is given a certain target stimuli~\cite{francombe2015model}.
This model was designed with the application of sound zones in mind and is as such an especially promising
for use in the evaluation of the proposed algorithm in \autoref{ch:results}.

To create the model, a listening test was performed where the participants were subjected to audio-on-audio interference.
The subjects were played a target audio stimulus they were instructed to focus listening to.
At the same time, an interferer audio stimulus was played to distract the participant from the target.
The participants were given a scale between 0 and 100 on which they were asked to rate how distracting the interference
was when listening to the target program, where a 100 indicates that the interferer ``overpowered'' the target audio~\cite{francombe2015model}.

The target-interferer stimuli pairs and corresponding ratings resulted in a dataset.
This dataset was then used to fit a model which predicted the distraction given novel a target-interferer stimuli pair.
The model consisted of taking a linear combination of 5 features that were computed from the stimuli~\cite{francombe2015model}.

Computing said features could, however, not be performed in real-time.
The reason for this was that the original distraction model is too computationally complex~\cite{ramo2017real}.
To this end, in 2017, R\"am\"o et al. proposed a version of the distraction model that could be run in real-time.
This was done by approximating the features of the original distraction model by less computationally complex alternatives.
The resulting real-time distraction model was found to be less precise but could be run in 0.04\% of the time of the 
original distraction model~\cite{ramo2017real}.

At face value, the real-time distraction model seems promising to optimize.
However, while easy to compute, the model is non-differentiable as the model uses
piecewise functions and non-convex due to taking the logarithm of the square of the input signals.
In addition to this, the model also performs operations that are difficult to express mathematically, such as counting the number of short-time blocks that exceed a certain threshold~\cite{ramo2017real}.

\subsection{Review of Perceptual Models used in Audio Coding}
\label{ch:perceptual:review:audio_coding}
The second class of perceptual models that are considered are the perceptual models used in audio coding.
Audio coding algorithms attempt to find a low-bitrate representation of an audio input signal, which is a 
form of lossy compression.
As such, audio coding algorithms typically introduces errors in doing so, 
which can be a detriment to the listening experience.

To minimize the impact of these errors, many audio coding algorithms use a perceptual model to quantify 
how disturbing the introduced distortions are~\cite{herre2019psychoacoustic}.
The perceptual model is used to introduce encoding errors in such a way that the audio output signal is minimally perceptually distinguishable from the audio input signal~\cite{taal2012low}.
This model typically takes the form of a distortion function which determines how audible the difference between a reference input audio signal and a distorted output audio signal is.
This function can be used to, for example, encode an input audio signal such that it has minimal distortion for a specified bitrate.

The perceptual models used in audio coding are promising for integration into a sound zone algorithm, as they are often tractable for optimization.
As stated, these perceptual models typically take the form of some distortion function that quantifies how perceptually disturbing the introduced artifacts are. 
One approach, for example, could be to define sound zone algorithms that minimize said distortion function.

\subsubsection{Review of Perceptual Models from ISO MPEG Standard}
The ISO/IEC 11172-3 standard specifies a coded representation for audio files~\cite{ISO11172-3}, 
and a corresponding decoder.
An encoder for said representation is not part of the standard.
This is done deliberately to allow for future improvements to the encoder without having to change the standard~\cite{pan1995tutorial}.

The standard does, however, provide a number of examples of possible encoders with increasing complexity.
Alongside these example encoders, two psycho-acoustical models are included for use during the encoding process. 

The psycho-acoustical models work by subdividing the input audio signal into frequency bands that correspond to the frequency bands in the human auditory system.
The model then determines how much quantization noise can be added separately per band without the noise becoming audible.
As such, the model assumes that the distortion signal is noise-like~\cite{van2005perceptual}, which is usually
the case for quantization noise for audio coders.

The output of the psycho-acoustical model is thus the amount of noise that can be added per band.
In the case of audio coding, this can then be used to control quantization noise.
Note that this perceptual model does not come in the form of the earlier described distortion function.
This technique has, however, been used for various signal processing purposes, such as audio watermarking~\cite{taal2012low}.
As such, examples exist from which optimization schemes could be inspired.

\subsubsection{Review of Par Distortion Detectability Measure}
In 2005, van der Par et al. proposed a novel perceptual model designed for use in audio coding~\cite{van2005perceptual}.
The model defines a distortion measure that determines the ``distortion detectability'' of a distortion signal in the presence of a masking signal.
That is to say, the function quantifies the degree to which a human is to detect a distortion signal while also listening to the masking signal.
For audio coding purposes, this distortion signal is the error introduced due to the audio compression.

Similarly to the ISO MPEG perceptual models, the Par detectability typically operates on short-time segments, 
typically in the order of 20 to 200 milliseconds~\cite{van2005perceptual}.
The proposed method, however, differentiates itself from the previously discussed ISO MPEG models in three ways.

Firstly, the paper uses newer findings from psycho-acoustic literature, namely spectral integration.
In spectral integration, the masking effects from neighboring bands are taken into account when computing the masking effects.
The psycho-acoustical models defined in the ISO MPEG standard does not do this as it effectively 
works independently per band~\cite{taal2012low}.

Secondly, it assumes that the distortion signal is sinusoidal rather than noise-like.
As such, it is more effective in hiding sinusoidal distortion.

Thirdly and finally, the perceptual model is described as a distortion function that quantifies how detectable a disturbance stimulus is.

The proposed distortion measure can be expressed as a squared $L^2$-norm, making it tractable for integration into existing least-square problems.
As such, the Par distortion detectability has been used in many signal processing applications, examples ranging from speech enhancement to removing perceptually irrelevant sinusoidal 
components~\cite{balazs2009time, taal2013optimal}.

\subsubsection{Review of Taal Distortion Detectability Measure}
A paper from 2012 by Taal et al. proposed a novel perceptual model \cite{taal2012low} which introduces a
alternative definition to the distortion detectability defined in the Par distortion detectability.

In contrast to the approach proposed by van der Par et al.~\cite{van2005perceptual},
the Taal distortion detectability measure takes temporal characteristics of the distortion and masking signals into account.
The inclusion of temporal information allows for the suppression of ``pre-echoes'', which is an artifact that 
the Par distortion detectability suffers from~\cite{taal2012low}. 
The ``pre-echoes'' artifacts arise from the assumption that the masking effects of the masking signal are stationary across time. 
As a result, audio coding algorithms may assume that audio content is masked while it is not, resulting in quantization noise not being masked.

In contrast to other temporal perceptual models, the Taal Detectability has a relatively low computational complexity.

The computational demand was, however, shown to be higher than the Par distortion detectability~\cite{taal2012low}, 
especially for longer time segments.
